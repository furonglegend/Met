# Baseline Experiment Scripts

EMMET åŸºçº¿å¤ç°ä¸è¯„æµ‹è„šæœ¬é›†åˆï¼Œæ”¯æŒ Memory Replay æœºåˆ¶ã€‚

## ğŸ“ è„šæœ¬è¯´æ˜

| æ–‡ä»¶ | åŠŸèƒ½ | ç”¨é€” | å¯¹åº” TODO |
|------|------|------|----------|
| `prepare_data.py` | æ•°æ®é‡‡æ ·å·¥å…· | ä»å®Œæ•´æ•°æ®é›†ä¸­é‡‡æ ·æŒ‡å®šæ•°é‡ | Phase 1.1 |
| `run_baseline.py` | **ä¸»å®éªŒè„šæœ¬** | è¿è¡Œå•ä¸ªç¼–è¾‘å®éªŒå¹¶è¯„æµ‹ | æ‰€æœ‰ Phase |
| `run_all_baselines.cmd/sh` | **ä¸‰å¤§åŸºçº¿å¯¹æ¯”** | ROME vs MEMIT vs EMMET | **Phase 1.2** |
| `run_batch_experiments.py` | æ‰¹é‡å®éªŒè¿è¡Œå™¨ | ç½‘æ ¼æœç´¢å¤šä¸ªé…ç½® | Phase 5.2 |
| `run_lora_ablation.cmd/sh` | **LoRA æ¶ˆèå®éªŒ** | æµ‹è¯•ä¸åŒ rank çš„å½±å“ | **Phase 3.2** |
| `run_combined_experiments.cmd` | **ç»„åˆé…ç½®å®éªŒ** | Replay + LoRA ç»„åˆæµ‹è¯• | **Phase 3.2** |
| `analyze_results.py` | ç»“æœåˆ†æè„šæœ¬ | èšåˆå’Œç»Ÿè®¡å®éªŒç»“æœ | Phase 5.3 |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¬¬1æ­¥: ä¸‰å¤§åŸºçº¿å¯¹æ¯”ï¼ˆPhase 1.2ï¼‰

```bash
# Windows
scripts\run_all_baselines.cmd

# Linux
bash scripts/run_all_baselines.sh
```

**ç›®æ ‡**: è¯æ˜ç»Ÿä¸€æ¡†æ¶çš„å¿…è¦æ€§ä¸ EMMET çš„ä¼˜åŠ¿

è¿è¡Œ 3 ä¸ªå®éªŒï¼š
- ROME: å•æ¡ç¼–è¾‘ï¼ˆbatch_size=1ï¼‰ï¼Œ200æ¡
- MEMIT: æ‰¹é‡ç¼–è¾‘ï¼ˆbatch_size=32ï¼‰ï¼Œ200æ¡
- EMMET: æ‰¹é‡ç¼–è¾‘ï¼ˆbatch_size=32ï¼‰ï¼Œ200æ¡

**è¾“å‡º**: `results/baseline_comparison/` + `baseline_comparison.csv`

### ç¬¬2æ­¥: Memory Replay å®éªŒï¼ˆPhase 2ï¼‰e 2ï¼‰

```bash
# å•ä¸ª Replay å®éªŒ
python scripts\run_baseline.py --method emmet --model gpt2 \
    --num_edits 200 --batch_size 32 --replay_rate 0.3 --seed 42
```

### ç¬¬3æ­¥: LoRA æ¶ˆèå®éªŒï¼ˆPhase 3ï¼‰

```bash
# Windows
scripts\run_lora_ablation.cmd

# Linux
bash scripts/run_lora_ablation.sh
```

æµ‹è¯•ä¸åŒ LoRA rankï¼ˆ4/8/16ï¼‰å¯¹æ€§èƒ½çš„å½±å“ã€‚

### ç¬¬4æ­¥: ç»„åˆé…ç½®å®éªŒ

```bash
# Windows
scripts\run_combined_experiments.cmd
```

æµ‹è¯• EMMET + Replay + LoRA çš„å„ç§ç»„åˆé…ç½®ã€‚

## ğŸ“Š å®éªŒçŸ©é˜µæ¦‚è§ˆ

### åŸºçº¿å¯¹æ¯”å®éªŒï¼ˆTODO 1.2ï¼‰

**ç›®æ ‡**: è¯æ˜ç»Ÿä¸€æ¡†æ¶å¿…è¦æ€§

| å®éªŒID | æ–¹æ³• | Batch Size | Num Edits | è¯´æ˜ |
|--------|------|------------|-----------|------|
| 1 | ROME | 1 | 200 | ä¼ ç»Ÿå•æ¡ç¼–è¾‘ |
| 2 | MEMIT | 32 | 200 | æ‰¹é‡æœ€å°äºŒä¹˜ |
| 3 | EMMET | 32 | 200 | ç»Ÿä¸€é—­å¼è§£ |

**è„šæœ¬**: `run_all_baselines.cmd`

### MVPå®éªŒçŸ©é˜µï¼ˆTODO Phase 2ï¼‰

**ç›®æ ‡**: éªŒè¯ Memory Replay ç¼“è§£é—å¿˜

æ ¹æ® TODO.md Phase 2ï¼Œæœ€å°å¯è¡Œå®éªŒåŒ…æ‹¬:

| å®éªŒID | æ–¹æ³• | Batch Size | Replay Rate | è¯´æ˜ |
|--------|------|------------|-------------|------|
| 1 | EMMET | 1 | 0.0 | åŸºçº¿-å•æ¡ç¼–è¾‘ |
| 2 | EMMET | 32 | 0.0 | åŸºçº¿-ä¸­ç­‰æ‰¹é‡ |
| 3 | EMMET | 256 | 0.0 | åŸºçº¿-å¤§æ‰¹é‡ |
| 4 | EMMET | 1 | 0.3 | Replay-å•æ¡ç¼–è¾‘ |
| 5 | EMMET | 32 | 0.3 | Replay-ä¸­ç­‰æ‰¹é‡ |
| 6 | EMMET | 256 | 0.3 | Replay-å¤§æ‰¹é‡ |

**å›ºå®šå‚æ•°**:
- Model: GPT-2 (774M)
- Num edits: 500
- Seed: 42

## ğŸ”§ è„šæœ¬è¯¦è§£

### 1. minimal_test.py - ç¯å¢ƒéªŒè¯

```bash
python scripts/minimal_test.py
```

**æ£€æŸ¥é¡¹**:
1. Python ç‰ˆæœ¬ (3.9)
2. PyTorch + CUDA
3. Transformers
4. å…¶ä»–ä¾èµ– (numpy, pandas, scipy)
5. æ•°æ®æ–‡ä»¶
6. GPT-2 æ¨¡å‹åŠ è½½
7. é¡¹ç›®ç»“æ„
8. æ¨¡å—å¯¼å…¥
9. æ•°æ®æ ¼å¼

### 2. run_baseline.py - ä¸»å®éªŒè„šæœ¬

**å®Œæ•´å‚æ•°**:

```bash
python scripts/run_baseline.py \
    --method emmet \              # ç¼–è¾‘æ–¹æ³•: emmet/memit/rome
    --model gpt2 \                # æ¨¡å‹: gpt2/gpt2-xl/llama3.2-3b
    --num_edits 500 \             # ç¼–è¾‘æ•°é‡
    --batch_size 32 \             # æ‰¹é‡å¤§å°
    --replay_rate 0.0 \           # Replayæ¯”ä¾‹ (0-1)
    --use_lora \                  # å¯ç”¨ LoRA (å¯é€‰)
    --lora_rank 8 \               # LoRA rank (é»˜è®¤8)
    --lora_alpha 16 \             # LoRA alpha (é»˜è®¤16)
    --seed 42 \                   # éšæœºç§å­
    --dataset counterfact_sampled_unique_cf_10_20000 \  # æ•°æ®é›†
    --output_dir results/baseline  # è¾“å‡ºç›®å½•
```

**è¾“å‡ºç»“æ„**:

```
results/baseline/emmet_gpt2_b32_replay0.0_20231113_143052/
â”œâ”€â”€ config.json              # å®éªŒé…ç½®
â”œâ”€â”€ experiment.log           # è¯¦ç»†æ—¥å¿—
â”œâ”€â”€ edit_results.json        # ç¼–è¾‘è¿‡ç¨‹ç»“æœ
â”œâ”€â”€ detailed_results.json    # æ¯æ¡æ•°æ®çš„è¯„æµ‹ç»“æœ
â”œâ”€â”€ detailed_results.csv     # CSVæ ¼å¼
â”œâ”€â”€ metrics.json             # èšåˆæŒ‡æ ‡ (ES/PS/NS/S)
â””â”€â”€ metrics.csv              # CSVæ ¼å¼
```

**è¯„æµ‹æŒ‡æ ‡**:

| æŒ‡æ ‡ | ç¼©å†™ | è®¡ç®—æ–¹å¼ | å«ä¹‰ |
|------|------|----------|------|
| Efficacy Score | ES | æµ‹è¯• rewrite prompt | ç¼–è¾‘æˆåŠŸç‡ |
| Paraphrase Score | PS | æµ‹è¯• paraphrase prompts | æ³›åŒ–èƒ½åŠ› |
| Neighborhood Specificity | NS | æµ‹è¯• neighborhood prompts | çŸ¥è¯†å±€éƒ¨æ€§ |
| Composite Score | S | (ES+PS+NS)/3 | ç»¼åˆå¾—åˆ† |

### 2. run_all_baselines.cmd - ä¸‰å¤§åŸºçº¿å¯¹æ¯”ï¼ˆTODO 1.2ï¼‰

```bash
# Windows
scripts\run_all_baselines.cmd

# Linux (åˆ›å»ºå¯¹åº”çš„ .sh ç‰ˆæœ¬)
bash scripts/run_all_baselines.sh
```

**ç›®æ ‡**: è¯æ˜ç»Ÿä¸€æ¡†æ¶çš„å¿…è¦æ€§ä¸ EMMET çš„ä¼˜åŠ¿

**å®éªŒé…ç½®**:
- Model: GPT-2 XL (1.5B)
- Num edits: 200
- Seed: 42
- ROME: batch_size=1ï¼ˆå•æ¡ç¼–è¾‘ï¼‰
- MEMIT: batch_size=32ï¼ˆæ‰¹é‡ç¼–è¾‘ï¼‰
- EMMET: batch_size=32ï¼ˆæ‰¹é‡ç¼–è¾‘ï¼‰

**å¯¹æ¯”ç»´åº¦**:
1. **Efficacy Score (ES)**: ç¼–è¾‘æˆåŠŸç‡
2. **Paraphrase Score (PS)**: æ³›åŒ–èƒ½åŠ›
3. **Neighborhood Specificity (NS)**: çŸ¥è¯†å±€éƒ¨æ€§
4. **æ—¶é—´ä¸æ˜¾å­˜å¼€é”€**: æ•ˆç‡å¯¹æ¯”

**è¾“å‡ºç»“æ„**:
```
results/baseline_comparison/
â”œâ”€â”€ rome_gpt2-xl_b1_20231114_*/     # ROME ç»“æœ
â”œâ”€â”€ memit_gpt2-xl_b32_20231114_*/   # MEMIT ç»“æœ
â”œâ”€â”€ emmet_gpt2-xl_b32_20231114_*/   # EMMET ç»“æœ
â””â”€â”€ baseline_comparison.csv          # èšåˆå¯¹æ¯”è¡¨
```

**å…³é”®ç‚¹**ï¼ˆå¯¹åº” TODO 1.2ï¼‰:
- âœ… ä½¿ç”¨ç›¸åŒæ•°æ®é›†ä¸éšæœºç§å­
- âœ… å¯¹é½è¯„æµ‹æŒ‡æ ‡å®ç°
- âœ… ä¿å­˜ä¸­é—´ç¼–è¾‘çŠ¶æ€ä»¥ä¾›åç»­åˆ†æ
- âœ… è®°å½•æ—¶é—´ä¸æ˜¾å­˜å¼€é”€

### 3. run_mvp_experiments.cmd - MVPå®éªŒçŸ©é˜µï¼ˆTODO Phase 2ï¼‰

```bash
# Windows
scripts\run_mvp_experiments.cmd

# Linux
bash scripts/run_mvp_experiments.sh
```

**ç›®æ ‡**: éªŒè¯ Memory Replay ç¼“è§£é—å¿˜

**å®éªŒçŸ©é˜µ**: 2ç§é…ç½® Ã— 3ç§æ‰¹é‡å¤§å° = 6ç»„å®éªŒ
- EMMET baseline (replay_rate=0.0)
- EMMET + Replay (replay_rate=0.3)
- Batch sizes: 1, 32, 256

**å›ºå®šå‚æ•°**:
- Model: GPT-2 (774M)
- Num edits: 500
- Seed: 42

**è¾“å‡º**: `results/baseline/` + é—å¿˜æ›²çº¿æ•°æ®

---

## ğŸ”§ LoRA é›†æˆ (Phase 3)

### LoRA æ¦‚è¿°

**Low-Rank Adaptation (LoRA)** æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œåœ¨ EMMET ç¼–è¾‘ååº”ç”¨ã€‚

**æ ¸å¿ƒç‰¹æ€§**:
- **åå¤„ç†å¼æ¶æ„**: LoRA åœ¨ EMMET ç¼–è¾‘å®Œæˆååº”ç”¨ï¼Œä¸ä¿®æ”¹é—­å¼è§£
- **ä½ç§©åˆ†è§£**: W' = W_base + (Î±/r) * B @ A
- **å‚æ•°é«˜æ•ˆ**: ä»…å¢åŠ  rÃ—(d_in + d_out) ä¸ªå¯è®­ç»ƒå‚æ•°ï¼ˆ<1%ï¼‰

### ä½¿ç”¨æ–¹æ³•

#### 1. åŸºæœ¬ç”¨æ³•

```bash
# EMMET + LoRA
python scripts/run_baseline.py \
    --method emmet \
    --model gpt2 \
    --num_edits 100 \
    --batch_size 10 \
    --use_lora \
    --lora_rank 8 \
    --lora_alpha 16
```

#### 2. ç»„åˆä½¿ç”¨

```bash
# EMMET + Memory Replay + LoRA
python scripts/run_baseline.py \
    --method emmet \
    --model gpt2 \
    --num_edits 200 \
    --batch_size 16 \
    --replay_rate 0.3 \
    --use_lora \
    --lora_rank 8
```

### LoRA å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--use_lora` | False | æ˜¯å¦å¯ç”¨ LoRA |
| `--lora_rank` | 8 | ä½ç§©åˆ†è§£çš„ç§©ï¼ˆæ¨è: 4/8/16ï¼‰ |
| `--lora_alpha` | 16.0 | ç¼©æ”¾å› å­ï¼ˆé€šå¸¸ä¸º 2Ã—rankï¼‰ |

### LoRA å®éªŒè„šæœ¬

#### run_lora_ablation.cmd/sh - LoRA æ¶ˆèå®éªŒ

æµ‹è¯•ä¸åŒ rank å¯¹æ€§èƒ½çš„å½±å“ï¼š

```bash
# Windows
scripts\run_lora_ablation.cmd

# Linux
bash scripts/run_lora_ablation.sh
```

**å®éªŒé…ç½®**:
- EMMET baseline (no LoRA)
- EMMET + LoRA rank=4 (Î±=8)
- EMMET + LoRA rank=8 (Î±=16)
- EMMET + LoRA rank=16 (Î±=32)

**å›ºå®šå‚æ•°**: MODEL=gpt2, NUM_EDITS=100, BATCH_SIZE=10, SEED=42

#### run_combined_experiments.cmd - ç»„åˆé…ç½®å®éªŒ

æµ‹è¯•æ‰€æœ‰ç»„åˆé…ç½®ï¼š

```bash
scripts\run_combined_experiments.cmd
```

**åŒ…å« 7 ç§é…ç½®**:
1. EMMET baseline
2. EMMET + Replay (0.3)
3. EMMET + LoRA (rank=8)
4. EMMET + Replay (0.3) + LoRA (rank=8)
5. EMMET + Replay (0.5) + LoRA (rank=4)
6. EMMET + Replay (0.3) + LoRA (rank=16)
7. EMMET + Replay (0.1) + LoRA (rank=8)

### LoRA æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ | é»˜è®¤ç›®æ ‡æ¨¡å— |
|------|--------------|
| GPT-2 | `mlp.c_fc`, `mlp.c_proj` |
| LLaMA | `mlp.up_proj`, `mlp.down_proj`, `mlp.gate_proj` |
| GPT-J | `mlp.fc_in`, `mlp.fc_out` |
| OPT | `fc1`, `fc2` |

### LoRA å‚æ•°æ•ˆç‡

ä»¥ GPT-2 (124M) ä¸ºä¾‹ï¼š

| Rank | LoRA å‚æ•° | å æ¯” | è®­ç»ƒå‚æ•°å‡å°‘ |
|------|-----------|------|--------------|
| 4 | ~0.3M | 0.24% | 99.76% |
| 8 | ~0.6M | 0.48% | 99.52% |
| 16 | ~1.2M | 0.97% | 99.03% |

### LoRA API å‚è€ƒ

```python
from emmet.lora_wrapper import apply_lora_to_edited_model

lora_wrapper = apply_lora_to_edited_model(
    model=edited_model,              # EMMET ç¼–è¾‘åçš„æ¨¡å‹
    target_modules=['mlp.c_fc', 'mlp.c_proj'],  # ç›®æ ‡æ¨¡å—
    rank=8,                          # LoRA rank
    alpha=16.0,                      # ç¼©æ”¾å› å­
    freeze_base=True                 # å†»ç»“åŸºç¡€å‚æ•°
)

# è·å–å‚æ•°ç»Ÿè®¡
stats = lora_wrapper.get_param_count()

# å¯ç”¨/ç¦ç”¨ LoRA
lora_wrapper.enable_lora()
lora_wrapper.disable_lora()

# åˆå¹¶ LoRA åˆ°åŸºç¡€æƒé‡
lora_wrapper.merge_lora()
```

### LoRA æ•…éšœæ’é™¤

**é—®é¢˜ï¼šæ˜¾å­˜ä¸è¶³**
- å‡å° LoRA rank (8 â†’ 4)
- å‡å° batch_size
- å‡å°‘ target_modules æ•°é‡

**é—®é¢˜ï¼šæ€§èƒ½ä¸‹é™**
- å¢åŠ  rank (8 â†’ 16)
- è°ƒæ•´ alpha = 2 Ã— rank
- è¿è¡Œæ¶ˆèå®éªŒæ‰¾åˆ°æœ€ä½³é…ç½®

---

### 4. run_batch_experiments.py - æ‰¹é‡å®éªŒè¿è¡Œå™¨ï¼ˆTODO 4.2ï¼‰

**ä½¿ç”¨é…ç½®æ–‡ä»¶**:

```json
{
  "methods": ["emmet"],
  "models": ["gpt2"],
  "num_edits_list": [500],
  "batch_sizes": [1, 32, 256],
  "replay_rates": [0.0, 0.3],
  "seeds": [42],
  "dataset": "counterfact_sampled_unique_cf_10_20000",
  "output_dir": "results/baseline"
}
```

**è¿è¡Œ**:

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
python scripts/run_batch_experiments.py --config configs/full_experiment_config.json

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
python scripts/run_batch_experiments.py \
    --methods emmet \
    --models gpt2 \
    --num_edits 500 \
    --batch_sizes 1 32 256 \
    --replay_rates 0.0 0.3 \
    --seeds 42
```

è‡ªåŠ¨è¿è¡Œæ‰€æœ‰å‚æ•°ç»„åˆ (2Ã—3=6 ç»„å®éªŒ)ã€‚

### 5. analyze_results.py - ç»“æœåˆ†æï¼ˆTODO 4.3ï¼‰

```bash
python scripts/analyze_results.py \
    --results_dir results/baseline \
    --output aggregated_results
```

**è¾“å‡º**:
- `aggregated_results.csv`: æ‰€æœ‰å®éªŒçš„è¯¦ç»†ç»“æœ
- `statistics.csv`: æŒ‰æ–¹æ³•ã€æ‰¹é‡ã€Replayç‡åˆ†ç»„çš„ç»Ÿè®¡

**åˆ†ç»„ç»´åº¦**:
- Method (emmet/memit/rome)
- Batch Size (1/32/256)
- Replay Rate (0.0/0.3)

### 6. prepare_data.py - æ•°æ®é‡‡æ ·ï¼ˆTODO 1.1ï¼‰

```bash
# é‡‡æ · 200 æ¡
python scripts/prepare_data.py --num 200 --seed 42

# é‡‡æ ·åˆ°è‡ªå®šä¹‰æ–‡ä»¶
python scripts/prepare_data.py --num 500 --seed 42 --output data/sample_500.json
```

## ğŸ“ˆ å®éªŒå·¥ä½œæµï¼ˆå¯¹åº” TODO.mdï¼‰

### Phase 0 (Day 0) - ç¯å¢ƒå‡†å¤‡ä¸éªŒè¯

**ç›®æ ‡**: ç¡®ä¿æŠ€æœ¯æ ˆå¯è¡Œæ€§

```bash
# 1. ç¯å¢ƒéªŒè¯
python scripts/minimal_test.py

# 2. å¿«é€Ÿæµ‹è¯•ï¼ˆ10æ¡æ•°æ®ï¼‰
scripts\quick_test.cmd
```

**äº§å‡º**: ç¯å¢ƒéªŒè¯é€šè¿‡ + å¿«é€Ÿæµ‹è¯•ç»“æœ

---

### Phase 1 (TODO 1.2) - ä¸‰å¤§åŸºçº¿å¯¹æ¯”

**ç›®æ ‡**: è¯æ˜ç»Ÿä¸€æ¡†æ¶å¿…è¦æ€§

```bash
# è¿è¡Œ ROME vs MEMIT vs EMMET å¯¹æ¯”
scripts\run_all_baselines.cmd

# åˆ†æç»“æœ
python scripts\analyze_results.py --results_dir results/baseline_comparison
```

**äº§å‡º**: `baseline_comparison.csv` + å¯¹æ¯”åˆ†ææŠ¥å‘Š

**å…³é”®å‘ç°**:
- ROME: ç²¾ç¡®ä½†æ…¢ï¼ˆå•æ¡ç¼–è¾‘ï¼‰
- MEMIT: å¿«é€Ÿä½†è¿‘ä¼¼ï¼ˆæœ€å°äºŒä¹˜æ¾å¼›ï¼‰
- EMMET: å¹³è¡¡æ•ˆç‡ä¸ç²¾åº¦ï¼ˆé—­å¼è§£ï¼‰

---

### Phase 2 (TODO Phase 2) - Memory Replay éªŒè¯

**ç›®æ ‡**: éªŒè¯ Replay æœºåˆ¶ç¼“è§£é—å¿˜

```bash
# è¿è¡Œ MVP å®éªŒçŸ©é˜µï¼ˆ6ç»„ï¼‰
scripts\run_mvp_experiments.cmd

# åˆ†æé—å¿˜æ›²çº¿
python scripts\analyze_results.py --results_dir results/baseline
```

**äº§å‡º**: é—å¿˜æ›²çº¿å›¾ + Replay æ•ˆæœåˆ†æ

---

### Phase 3 (TODO 4.2) - å¤§è§„æ¨¡æ¶ˆèå®éªŒ

**ç›®æ ‡**: ç³»ç»Ÿè¯„æµ‹å„é…ç½®ç»„åˆ

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è¿è¡Œå®Œæ•´å®éªŒçŸ©é˜µ
python scripts/run_batch_experiments.py --config configs/full_experiment_config.json
```

**äº§å‡º**: å®Œæ•´å®éªŒçŸ©é˜µç»“æœ

---

## ğŸ“… å®éªŒè¿›åº¦è¿½è¸ªï¼ˆåŸºäº TODO.mdï¼‰

### âœ… Phase 0: çŸ¥è¯†å‡†å¤‡ä¸ç¯å¢ƒé…ç½®

- [x] ç¯å¢ƒé…ç½® (conda + PyTorch + Transformers)
- [x] æ•°æ®é›†å‡†å¤‡ (CounterFact)
- [x] ç¯å¢ƒéªŒè¯è„šæœ¬ (`minimal_test.py`)
- [x] å¿«é€Ÿæµ‹è¯•è„šæœ¬ (`quick_test.cmd`)

### ğŸ”„ Phase 1: åŸºçº¿å®éªŒä¸å¯¹æ¯” [P0 ä¼˜å…ˆçº§]

**1.1 å°è§„æ¨¡å¿«é€ŸéªŒè¯ï¼ˆ200-500æ¡ï¼‰**
- [x] å‡†å¤‡ CounterFact å­é›†
- [ ] è¿è¡Œ EMMET æœ€å°ç¤ºä¾‹
- [ ] ç¡®è®¤ ES/PS/NS æŒ‡æ ‡è®¡ç®—æ­£ç¡®
- [ ] è°ƒè¯•è¶…å‚æ•°

**1.2 ä¸‰å¤§åŸºçº¿å¯¹æ¯”å®éªŒï¼ˆROME / MEMIT / EMMETï¼‰**
- [x] åˆ›å»º `run_all_baselines.cmd` è„šæœ¬
- [ ] ROME: å•æ¡ç¼–è¾‘ï¼ˆbatch_size=1ï¼‰ï¼Œ200æ¡
- [ ] MEMIT: æ‰¹é‡ç¼–è¾‘ï¼ˆbatch_size=32ï¼‰ï¼Œ200æ¡
- [ ] EMMET: æ‰¹é‡ç¼–è¾‘ï¼ˆbatch_size=32ï¼‰ï¼Œ200æ¡
- [ ] å¯¹æ¯”ä¸‰è€…çš„ ES/PS/NS å·®å¼‚
- [ ] è®°å½•æ—¶é—´ä¸æ˜¾å­˜å¼€é”€

**äº§å‡º**: `results/baseline_comparison.csv`

### â³ Phase 2: Memory Replay å®ç° [P1 æ ¸å¿ƒè´¡çŒ®]

**2.1 Replay Buffer è®¾è®¡ä¸å®ç°**
- [ ] è®¾è®¡ Buffer æ•°æ®ç»“æ„
- [ ] å®ç°é‡‡æ ·ç­–ç•¥
- [ ] å®ç° Buffer ç»´æŠ¤

**2.2 é›†æˆåˆ° EMMET é—­å¼è§£**
- [ ] åœ¨æ„å»ºçº¦æŸæ—¶æ‹¼æ¥å½“å‰æ‰¹ + å†å²é‡‡æ ·æ‰¹
- [ ] æ•°å€¼ç¨³å®šæ€§å¤„ç†

**2.3 å°è§„æ¨¡æ¶ˆèå®éªŒ**
- [ ] Replay Rate æ¶ˆèï¼šr âˆˆ {0, 0.1, 0.3, 0.5}
- [ ] Buffer Size æ¶ˆè
- [ ] é‡‡æ ·ç­–ç•¥å¯¹æ¯”

### â³ Phase 3: æœ€å°åŒ– LoRA é›†æˆ [P2 æ»¡è¶³æŠ¥å‘Šæ‰¿è¯º]

- [ ] å®ç°æœ€å° LoRA Wrapper ç±»
- [ ] å°è§„æ¨¡å®éªŒï¼šEMMET vs EMMET+LoRA
- [ ] ä¸ Replay ç»„åˆéªŒè¯

### â³ Phase 4: ä¸­å¤§è§„æ¨¡ç³»ç»Ÿå®éªŒ [P3 è¯æ˜æœ‰æ•ˆæ€§]

**4.1 æ‰©å±•åˆ°ä¸­è§„æ¨¡æ•°æ®é›†ï¼ˆ2000-5000æ¡ï¼‰**
- [ ] è§‚å¯Ÿæ¸è¿›é—å¿˜ â†’ ç¾éš¾é—å¿˜çš„è½¬æŠ˜ç‚¹
- [ ] å¤šç§é…ç½®å¯¹æ¯”

**4.2 æ‰¹é‡è§„æ¨¡æ¶ˆèå®éªŒ**
- [ ] æ‰¹é‡å¤§å°ï¼š{1, 8, 32, 128, 512, 1024}
- [ ] Replay æ¯”ä¾‹ï¼šr âˆˆ {0, 0.1, 0.3, 0.5}
- [ ] éšæœºç§å­ï¼š{1, 2, 3}

**4.3 å¯è§†åŒ–ä¸åˆ†æ**
- [ ] é—å¿˜æ›²çº¿å›¾
- [ ] æ‰¹é‡è§„æ¨¡å¯¹æ¯”å›¾
- [ ] Replay æ•ˆæœçƒ­åŠ›å›¾

### â³ Phase 5: æŠ¥å‘Šæ’°å†™ä¸æ–‡æ¡£æ•´ç† [P4 æœ€ç»ˆäº¤ä»˜]

- [ ] æŠ€æœ¯æŠ¥å‘Šæ’°å†™ï¼ˆACL æ ¼å¼ï¼‰
- [ ] ä»£ç æ–‡æ¡£ä¸å¯å¤ç°æ€§
- [ ] å®éªŒæ—¥å¿—ä¸ç»“æœå½’æ¡£

---

## ğŸ—“ï¸ å·²å®Œæˆå®éªŒè®°å½•

### Day 0 (11/13) - ç¯å¢ƒå‡†å¤‡ [âœ… å®Œæˆ]

```bash
# 1. éªŒè¯ç¯å¢ƒ
python scripts/minimal_test.py

# 2. å¿«é€Ÿæµ‹è¯•
python scripts/test_baseline.py
# æˆ–ä½¿ç”¨ä¾¿æºè„šæœ¬
scripts\quick_test.cmd  # Windows
bash scripts/quick_test.sh  # Linux

# 3. å‡†å¤‡æ•°æ®ï¼ˆå¯é€‰ï¼‰
python scripts/prepare_data.py --num 500 --seed 42
```

### Day 1-2 (11/14-15) - EMMETåŸºçº¿

```bash
# é€‰é¡¹1: è¿è¡Œå®Œæ•´MVPçŸ©é˜µ
scripts\run_mvp_experiments.cmd  # Windows
bash scripts/run_mvp_experiments.sh  # Linux

# é€‰é¡¹2: æ‰‹åŠ¨è¿è¡Œå•ä¸ªå®éªŒ
python scripts/run_baseline.py --method emmet --model gpt2 --num_edits 500 --batch_size 32 --seed 42

# é€‰é¡¹3: æ‰¹é‡é…ç½®
python scripts/run_batch_experiments.py --config configs/full_experiment_config.json
```

### Day 3-4 (11/16-17) - Memory Replay

**éœ€è¦å®ç°**:
1. åˆ›å»º `src/emmet/replay_buffer.py`
2. ä¿®æ”¹ `src/emmet/emmet_main.py` é›†æˆ Replay
3. è¿è¡Œå¯¹æ¯”å®éªŒ:

```bash
# Replayå®éªŒï¼ˆreplay_rate=0.3å·²åœ¨MVPçŸ©é˜µä¸­ï¼‰
python scripts/run_baseline.py --method emmet --model gpt2 --num_edits 500 --batch_size 32 --replay_rate 0.3 --seed 42
```

### Day 5 (11/18) - ç»“æœåˆ†æä¸æŠ¥å‘Š

```bash
# 1. èšåˆç»“æœ
python scripts/analyze_results.py --results_dir results/baseline

# 2. æŸ¥çœ‹ç»Ÿè®¡
# æ‰“å¼€ results/baseline/statistics.csv

# 3. ç”Ÿæˆå›¾è¡¨ï¼ˆéœ€é¢å¤–è„šæœ¬ï¼‰
# ç»˜åˆ¶ ES/PS/NS å¯¹æ¯”
# ç»˜åˆ¶ Batch Size å½±å“
# ç»˜åˆ¶ Replay Rate å½±å“
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒè¦æ±‚

- Python: 3.9.7
- PyTorch: 1.12.1 (CUDA 11.3)
- Transformers: 4.23.1
- CUDA: 11.3 (å¯é€‰ï¼Œæ¨è)
- GPU æ˜¾å­˜: 2GB+ (GPT-2), 6GB+ (GPT-2-XL)

### æ€§èƒ½é¢„ä¼°

| é…ç½® | æ—¶é—´ | æ˜¾å­˜ |
|------|------|------|
| 500æ¡, batch=1, GPT-2 | ~5å°æ—¶ | 2GB |
| 500æ¡, batch=32, GPT-2 | ~1å°æ—¶ | 4GB |
| 500æ¡, batch=256, GPT-2 | ~30åˆ†é’Ÿ | 8GB |

**åŠ é€Ÿå»ºè®®**:
- ä½¿ç”¨æ›´å¤§çš„ batch_size (å¦‚æœæ˜¾å­˜å…è®¸)
- ä½¿ç”¨ CUDA (æ¯” CPU å¿« 10-50å€)
- å‡å°‘ num_edits ç”¨äºå¿«é€Ÿæµ‹è¯•

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: CUDA out of memory

```bash
# è§£å†³æ–¹æ¡ˆ1: å‡å°æ‰¹é‡
python scripts/run_baseline.py ... --batch_size 1

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨CPUï¼ˆWindowsï¼‰
set CUDA_VISIBLE_DEVICES=-1
python scripts/run_baseline.py ...

# è§£å†³æ–¹æ¡ˆ2: ä½¿ç”¨CPUï¼ˆLinuxï¼‰
CUDA_VISIBLE_DEVICES=-1 python scripts/run_baseline.py ...
```

### é—®é¢˜2: ModuleNotFoundError

```bash
# ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•
cd d:\Projects\nlp_final_project\emmet-stability-replay  # Windows
cd /path/to/emmet-stability-replay  # Linux

# æ£€æŸ¥condaç¯å¢ƒ
conda env list
conda activate emmet-edit
```

### é—®é¢˜3: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°

```bash
# æ£€æŸ¥æ•°æ®æ–‡ä»¶
dir data\counterfact_sampled_unique_cf_10_20000.json  # Windows
ls data/counterfact_sampled_unique_cf_10_20000.json  # Linux

# å¦‚æœç¼ºå¤±ï¼Œä½¿ç”¨prepare_data.pyç”Ÿæˆæ ·æœ¬
python scripts/prepare_data.py --num 500 --seed 42
```

### é—®é¢˜4: æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
python scripts/download_models.py

# æˆ–ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com  # Linux
set HF_ENDPOINT=https://hf-mirror.com  # Windows
```

### é—®é¢˜5: ReplayåŠŸèƒ½ä¸å¯ç”¨

**é¢„æœŸè¡Œä¸º**: Day 3-4 ä¹‹å‰ï¼Œ`--replay_rate` å‚æ•°ä¼šè¢«è®°å½•ä½†ä¸ç”Ÿæ•ˆã€‚

**è§£å†³**: æŒ‰ todo.md è®¡åˆ’ï¼Œåœ¨ Day 3-4 å®ç° `src/emmet/replay_buffer.py`ã€‚

## ğŸ“ å®ç°æ¸…å•

### âœ… å·²å®Œæˆ (Day 0)

- [x] `minimal_test.py` - ç¯å¢ƒéªŒè¯è„šæœ¬
- [x] `run_baseline.py` - ä¸»å®éªŒè„šæœ¬ (å®Œæ•´è¯„æµ‹é€»è¾‘)
- [x] `prepare_data.py` - æ•°æ®é‡‡æ ·å·¥å…·
- [x] `test_baseline.py` - å¿«é€Ÿæµ‹è¯•è„šæœ¬
- [x] `run_batch_experiments.py` - æ‰¹é‡å®éªŒè¿è¡Œå™¨
- [x] `analyze_results.py` - ç»“æœåˆ†æè„šæœ¬
- [x] `quick_test.cmd/sh` - ä¾¿æºæµ‹è¯•è„šæœ¬
- [x] `run_mvp_experiments.cmd/sh` - MVPå®éªŒçŸ©é˜µè„šæœ¬
- [x] æ–‡æ¡£åˆå¹¶ (README.md)

### â° å¾…å®ç° (Day 1-5)

- [ ] Day 1-2: è¿è¡ŒEMMETåŸºçº¿å®éªŒ (500æ¡)
- [ ] Day 3-4: å®ç° Memory Replay
  - [ ] `src/emmet/replay_buffer.py`
  - [ ] ä¿®æ”¹ `src/emmet/emmet_main.py`
- [ ] Day 3-4: è¿è¡Œ Replay å¯¹æ¯”å®éªŒ
- [ ] Day 5: ç»“æœå¯è§†åŒ–è„šæœ¬
- [ ] Day 5: æ’°å†™å®éªŒæŠ¥å‘Š

## ğŸ”— ç›¸å…³æ–‡æ¡£

- `todo.md` - é¡¹ç›®æ—¶é—´çº¿å’Œä»»åŠ¡æ¸…å•
- `docs/experiment_scripts.md` - å®éªŒè„šæœ¬è¯¦ç»†æ–‡æ¡£
- `docs/init_guide.md` - åˆå§‹åŒ–æŒ‡å—
- `configs/full_experiment_config.json` - å®éªŒé…ç½®ç¤ºä¾‹

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

### 1. å®Œæ•´çš„è¯„æµ‹æŒ‡æ ‡

- âœ… ES (Efficacy Score) - ç¼–è¾‘æˆåŠŸç‡
- âœ… PS (Paraphrase Score) - æ³›åŒ–èƒ½åŠ›
- âœ… NS (Neighborhood Specificity) - çŸ¥è¯†å±€éƒ¨æ€§
- âœ… S (Composite Score) - ç»¼åˆå¾—åˆ†

### 2. çµæ´»çš„æ‰¹é‡å¤„ç†

- æ”¯æŒ batch_size = 1, 32, 256 ç­‰
- è‡ªåŠ¨å¤„ç†ä¸èƒ½æ•´é™¤çš„æœ€åä¸€æ‰¹
- æ˜¾å­˜è‡ªé€‚åº”ï¼ˆbatchå¤§å°å½±å“æ˜¾å­˜ï¼‰

### 3. å¯å¤ç°æ€§ä¿è¯

- å›ºå®šéšæœºç§å­ (--seed 42)
- å®Œæ•´çš„å®éªŒé…ç½®ä¿å­˜
- è¯¦ç»†çš„è¿è¡Œæ—¥å¿—

### 4. å¤šæ ¼å¼è¾“å‡º

- JSON: è¯¦ç»†çš„åµŒå¥—ç»“æ„
- CSV: æ–¹ä¾¿ Excel/Pandas åˆ†æ
- æ—¥å¿—: å®æ—¶è°ƒè¯•ä¿¡æ¯

### 5. Memory Replay æ¥å£

- `--replay_rate` å‚æ•°é¢„ç•™
- Day 3-4 å®ç°åå³å¯ä½¿ç”¨
- æ— éœ€ä¿®æ”¹ä¸»å®éªŒè„šæœ¬

## ğŸ¤ è´¡çŒ®

è¿™æ˜¯ NLP è¯¾ç¨‹æœŸæœ«é¡¹ç›®çš„å®éªŒè„šæœ¬é›†åˆã€‚

**é¡¹ç›®ç›®æ ‡**: 
- å¤ç° EMMET åŸºçº¿
- å®ç° Memory Replay æœºåˆ¶
- å¯¹æ¯”æ‰¹é‡å¤§å°å’Œ Replay ç‡çš„å½±å“

**æˆªæ­¢æ—¥æœŸ**: 2025-11-18

---

**æœ€åæ›´æ–°**: 2025-11-13
**çŠ¶æ€**: Day 0 å®Œæˆï¼Œæ‰€æœ‰åŸºç¡€è„šæœ¬å°±ç»ª âœ…
